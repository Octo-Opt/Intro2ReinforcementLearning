{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview \n",
    "\n",
    "We will establish the state space, or we can think of this as the environment in which the agent will be trained. When an agent is in a state, it chooses an action based on a policy. There are 2 ways to set up policies:\n",
    "+ Deterministic: in each state, the value of the actions is specifically defined.\n",
    "+ Stochastic: in each state, the values of the action and state form a probability space.\n",
    "\n",
    "In this example, we will use a stochastic policy setting. At each state, the search space for the action will be evaluated according to a random function, we will take the largest value in that range to choose the behavior.\n",
    "\n",
    "Note: in this example, the main purpose is to describe the definitions in the environment and agent settings in reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_states = 4\n",
    "\n",
    "env = np.arange(n_states*n_states).reshape((n_states, n_states))\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0457407 , 0.23235773, 0.82253313, 0.0722362 ],\n",
       "       [0.7508166 , 0.74588432, 0.32160587, 0.34907592],\n",
       "       [0.91292341, 0.20002638, 0.81480425, 0.94573611],\n",
       "       [0.09645007, 0.2922429 , 0.61120022, 0.6111294 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = 4\n",
    "q_table = np.random.rand(n_states, n_actions)\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 0\n",
    "discount_factor = 0.01\n",
    "learning_rate = 0.9\n",
    "\n",
    "while True: \n",
    "    # To make sure that when the next state is bounded\n",
    "    if state + 1 >= n_states: \n",
    "        break\n",
    "\n",
    "    # From the current state, we would choose the action that has the highest score\n",
    "    action_from_state = q_table[state]\n",
    "    action = np.argmax(action_from_state)\n",
    "\n",
    "    currrent_q_value = q_table[state, action]\n",
    "    reward = env[state, action]\n",
    "    next_action = np.max(q_table[state+1])\n",
    "    target_q_value = reward + discount_factor*next_action\n",
    "\n",
    "    q_table[state, action] = learning_rate* (target_q_value - currrent_q_value)\n",
    "\n",
    "    state+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0457407 , 0.23235773, 1.05763179, 0.0722362 ],\n",
       "       [1.26470457, 0.74588432, 0.32160587, 0.34907592],\n",
       "       [0.91292341, 0.20002638, 0.81480425, 2.4133931 ],\n",
       "       [0.09645007, 0.2922429 , 0.61120022, 0.6111294 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
