{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from IPython.display import Image\n",
    "from matplotlib import animation\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem statement\n",
    "\n",
    "Given Taxi-v3 game environment, train an RL Agent to complete this game using Q-Learning. \n",
    "\n",
    "## Environment description\n",
    "\n",
    "+ Environment type: **Deterministic** environment. \n",
    "+ Action Space: Discrete(6). \n",
    "+ State Space: Discrete(500). \n",
    "+ Game Objective: Drop-off passenger to the right destination.\n",
    "+ Terminate State: Taxi drop-off passenger to the right destination or game timeout.\n",
    "+  Reward function:\n",
    "    + +20 points for a correct drop-off.\n",
    "    + -1 point for each move.\n",
    "    + -10 points for a wrong drop-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  500\n",
      "Action Space:  6\n"
     ]
    }
   ],
   "source": [
    "env_id = 'Taxi-v3'\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "\n",
    "state_space = env.observation_space.n \n",
    "action_space = env.action_space.n\n",
    "\n",
    "print('Observation Space: ', state_space)\n",
    "print('Action Space: ', action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_q_table(state_space, action_space): \n",
    "    q_table = np.zeros((state_space, action_space))\n",
    "    return q_table \n",
    "\n",
    "def greedy_policy(q_table, state): \n",
    "    action = np.argmax(q_table[state, :])\n",
    "    return action \n",
    "\n",
    "def epsilon_policy(q_table, state, epsilon): \n",
    "    \"\"\"\n",
    "        This policiy can ensure the balance between exploration and exploitation: \n",
    "            + Exploration: seeking to new knowledge about the environment by trying different actions and observe new state. \n",
    "            + Exploitation: Using knowledge agent already got to make actions that it expects will yield high rewards. \n",
    "    \"\"\"\n",
    "    rand_n = float(np.random.uniform(0, 1))\n",
    "\n",
    "    if rand_n > epsilon: \n",
    "        action = greedy_policy(q_table, state)\n",
    "    else: \n",
    "        action = np.random.choice(q_table.shape[1])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_episodes = 300\n",
    "n_eval_episodes = 10\n",
    "lr = 0.7\n",
    "\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "eval_seed = range(n_eval_episodes)\n",
    "\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.05\n",
    "decay_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, \n",
    "          q_table, \n",
    "          max_steps=max_steps, \n",
    "          n_training_episodes=n_training_episodes, \n",
    "          min_epsilon=min_epsilon, \n",
    "          max_epsilon=max_epsilon, \n",
    "          decay_rate=decay_rate, \n",
    "          lr=lr, \n",
    "          gamma=gamma):\n",
    "\n",
    "    for episode in tqdm(range(n_training_episodes), desc='Training model ...'): \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode) \n",
    "\n",
    "        state, info = env.reset()\n",
    "        step = 0\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        for step in range(max_steps): \n",
    "            action = epsilon_policy(q_table, state, epsilon)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            q_table[state, action] += lr * (reward + gamma*np.max(q_table[new_state]) - q_table[state, action])\n",
    "\n",
    "            if terminated or truncated: \n",
    "                break\n",
    "\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d598a0a77048f683143fd96aa8d646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training model ...:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_table = init_q_table(state_space, action_space)\n",
    "train_q_table = train(env, \n",
    "                      q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -3.94067474  -3.05795711  -4.03170362  -3.20909937  -2.9850844\n",
      "  -12.54565266]\n",
      " [ -2.80919142  -1.9447495   -2.8316826   -1.7178      -1.46543439\n",
      "  -10.4748    ]\n",
      " ...\n",
      " [ -3.10865425  -2.59361503  -3.19659993  -2.8024386  -11.49013232\n",
      "  -12.10897621]\n",
      " [ -3.99024184  -3.70766748  -4.01472188  -4.17976238 -13.1644032\n",
      "  -13.15448383]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table.shape)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, q_table, max_steps, seed): \n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(n_eval_episodes)): \n",
    "        if seed:\n",
    "            state, info = env.reset(seed=seed[episode])\n",
    "\n",
    "        else: \n",
    "            state, info = env.reset()\n",
    "\n",
    "        step, truncated, terminated = 0, False, False \n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps): \n",
    "            action = greedy_policy(q_table, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated: \n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    \n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7eb799fe7d455f9a5d7e075101909e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_agent(env, q_table, max_steps=max_steps, seed=eval_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-99.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(mean_reward)\n",
    "print(std_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
