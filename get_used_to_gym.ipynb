{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to get an acquaintance to the [gymnasium](https://gymnasium.farama.org/), one of the most popular libraries used to create environment for the deep learning model to interact with in the context of reinforcement learning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ reset(): observe an environment. In detail, after the environment is intialized, to make this environment works, the reset()-function is used. \n",
    "+ step(): the agent performs an action to the environment. After the action is done, the agent receives a new observation from the updated environment along with a reward for taking the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[observation] [-0.00442104  1.4048645  -0.44783115 -0.26915467  0.00512981  0.10144047\n",
      "  0.          0.        ] \n",
      "[info] {}\n",
      "Observation: [-0.00872402  1.3989621  -0.43595457 -0.26236492  0.01072471  0.11190945\n",
      "  0.          0.        ] \n",
      "Reward: 1.0964361823878448 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.01291962  1.3930277  -0.4257609  -0.26381418  0.016852    0.12255718\n",
      "  0.          0.        ] \n",
      "Reward: 0.47190225608728725 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.01720352  1.3864844  -0.43683314 -0.29092824  0.02519749  0.16692533\n",
      "  0.          0.        ] \n",
      "Reward: -2.6124642993433995 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.02139292  1.3793509  -0.42496476 -0.31715304  0.03115242  0.11910967\n",
      "  0.          0.        ] \n",
      "Reward: -0.46006766356148776 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.0256484   1.3716067  -0.43325153 -0.34437302  0.03877192  0.15240413\n",
      "  0.          0.        ] \n",
      "Reward: -2.3427507500397824 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.02998133  1.3632544  -0.44293937 -0.37148985  0.04833205  0.19122045\n",
      "  0.          0.        ] \n",
      "Reward: -2.6254545548252666 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.03443069  1.3552146  -0.45417756 -0.3576383   0.05747635  0.18290249\n",
      "  0.          0.        ] \n",
      "Reward: -0.4196988545331976 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.03894663  1.3465635  -0.46252304 -0.3849452   0.06829882  0.21646944\n",
      "  0.          0.        ] \n",
      "Reward: -2.6268052008169875 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.04363479  1.3384271  -0.47914404 -0.36212477  0.07854533  0.20494929\n",
      "  0.          0.        ] \n",
      "Reward: -0.4096143797505533 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n",
      "Observation: [-0.04824753  1.3297122  -0.4696536  -0.38778114  0.08685838  0.1662759\n",
      "  0.          0.        ] \n",
      "Reward: -0.8523591486045337 \n",
      "Terminated: False \n",
      "Truncated: False \n",
      "Info: {} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "print(f'[observation] {observation} \\n[info] {info}')\n",
    "for idx in range(10):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f'Observation: {observation} \\n' \\\n",
    "            f'Reward: {reward} \\n' \\\n",
    "            f'Terminated: {terminated} \\n' \\\n",
    "            f'Truncated: {truncated} \\n'\\\n",
    "            f'Info: {info} \\n')\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
       " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
       " 1.       ], (8,), float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "small_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
